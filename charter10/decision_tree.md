回归决策树
数据分为连续型数据和离散型数据，面对不同类型的数据，决策树分为两大类：分类决策树和回归决策树


根据数据类型的不同，分类决策树主要用于处理离散型数据，回归决策树主要用于处理连续性数据。


回归决策树的原理：
⼀个回归树对应着输入空间（即特征空间）的⼀个划分以及在划分单元上的输出值。分类树中，我们采用信息论中的方法，通过计算选择最佳划分点。

而在回归树中，采用的是启发式的方法。假如我们有n个特征，每个特征有si(i ∈ (1, n))个取值，那我们遍历所有特征， 尝试该特征所有取值，对空间进行划分，直到取到特征 j 的取值 s，使得损失函数最小，这样就得到了⼀个划分点。


回归决策树的算法描述：

回归决策树是一种机器学习算法，用于解决回归问题，即预测连续型变量的值。下面是回归决策树算法的基本描述：

1. 输入：训练数据集 D = {(x1, y1), (x2, y2), ..., (xm, ym)}，其中 xi 是输入样本的特征向量，yi 是对应的目标变量的值。
2. 输出：回归决策树模型 f(x)。
3. 算法步骤：
   a. 选择最优切分特征：通过选择一个切分特征和相应的切分点，将训练数据集分成两个子集。
   b. 对于每个子集，递归地调用步骤a，选择最优切分特征和切分点，继续划分子集，直到满足停止条件。
   c. 停止条件可以是达到最大深度、子集中样本数量小于阈值或其他预定义的条件。
   d. 在每个叶节点上，根据子集中目标变量的均值或其他统计量，生成叶节点的预测值。
   e. 构建回归决策树模型 f(x)。
4. 在预测阶段，通过将输入样本沿树的分支进行遍历，最终到达叶节点，并返回叶节点的预测值作为模型的输出。

回归决策树的核心思想是通过对特征空间进行递归的二分划分，构建一棵树形结构，从而将样本进行分段拟合。每个叶节点对应一个划分区域，根据该区域内样本的目标变量的均值或其他统计量来进行预测。算法通过选择最优切分特征和切分点来优化划分过程，使得每个子集内样本的目标变量尽可能地相似。

回归决策树具有可解释性强、能够处理连续型和离散型特征、适用于非线性关系等优点。然而，决策树容易过拟合和对输入数据的噪声敏感，因此通常需要采取剪枝、集成学习等技术来提高泛化能力和稳定性。


有例子可以演示！

转载文章：
https://blog.csdn.net/qq_46092061/article/details/118826347
侵权必删