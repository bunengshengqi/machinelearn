第10章  决策树
掌握决策树的数显过程
知道信息熵的公式和作用
知道信息增益、信息增益率和基尼系数的作用
知道ID3、C4.5、CART算法的区别
了解cart剪枝的作用
知道特征提取的作用
应用决策树分类算法实现决策树分类


决策树的划分依据（一）
信息增益：以某特征划分数据集前后的熵的差值。熵表示样本集合的不确定性，熵越大，样本的不确定性越大
因此，可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。
信息熵 = entroy（前）-entroy（后）


找一个案例，手动计算信息增益！！！


决策树的划分依据（二）
信息增益准则对可取值较多的数目有所偏好。为了减少这种偏好带来的不利影响
C4.5决策树算法，不直接利用信息增益，而是利用信息增益率来选择最优划分属性。


信息增益率：增益率是利用信息增益除以属性对应的固有值
使用信息增益率的好处有哪些？
减少数目偏好的影响；
后剪枝方法
对缺失值的处理


决策树的划分依据（三）
基尼值和基尼系数
CART是决策树分类算法，分类和回归任务都可以用

基尼值：从数据集D中随机抽取两个样本，其类别标志不一致的概率。
所以基尼值越小，数据集D的纯度就越高
基尼指数：一般，选择使划分后基尼系数最小的属性作为最优划分属性

C4.5不一定是二叉树，但是CART一定是二叉树



（拓展）
1.多变量决策树（概念，使用背景，代表算法）
2.决策树变量的两种类型
数字型：例如年收入，使用>=  >  <  <= 作为分割条件（排序后）
名称型：变量只能从有限的的选项中选数，如例子中的婚姻状况：只能是
"单身"，"已婚"，"离婚"，使用"="来分割。
3.如何评价分割点的好坏？



CART剪枝：
1 为什么要CART剪枝？
解决过拟合的问题
2 CART剪枝常用的方法
预剪枝和后剪枝


上述更新的部分!!!
6月11日




