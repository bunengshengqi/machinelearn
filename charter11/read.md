本章主要讲解：集成学习

1 了解什么是集成学习？

集成学习通过建立几个模型来解决单一预测的问题，它的工作原理是生成多个分类器、模型。各自独立的学习和做出预测，
这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。

2 知道机器学习中的两个核心任务

任务一：如何优化训练数据：主要用于解决欠拟合问题
任务二：如何提升泛化性能：主要用于解决过拟合问题

3 了解集成学习中的bosting和bagging
解决欠拟合问题
（1）：弱弱组合变强
（2）：bosting ---逐步增强学习

解决过拟合问题
（1）：互相遏制变壮
（2）：Bagging ---采样学习集成



bagging和随机森林

知道bagging的集成原理
知道随机森林的构造过程
知道什么是包外估计
知道RandomForestClassifier的使用
了解bagging集成的优点

bagging的过程：采样+学习+集成


随机森林的构造过程
随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定的

随机森林 = bagging（思想） +  决策树的算法


包外估计的用途
当基学习器是决策树时，可以使用包外样本来辅助剪枝，或用于估计决策树中各结点的后验概率
以辅助对零训练样本结点处理；

当基学习是神经网络时，可使用包外样本来辅助早期停止以减少过拟合。


随机森林介绍的API(suijisenli_demo)

bagging集成优点：
bagging + 决策树、线性回归、逻辑回归、深度学习、、、=bagging集成学习方法

经过上面方式组成的学习方法
1.均可在原有算法上提高2%的泛化正确率
2.简单、方便、通用。


